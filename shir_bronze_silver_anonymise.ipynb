{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97343b12-7ca7-44c9-811b-cd911a37a8f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pseudonymisation pipeline\n",
    "\n",
    "The pipeline encrypts the data using four functions:\n",
    "* Encrypt ID - concatenates the data using a salt key and encrypts this data using sha2, then removing original PII column\n",
    "* Encrypt Date - removes the day from dates of the format DD/MM/YYYY (Note: this can be adapted in the code if date format changes)\n",
    "* Encrypt Timestamp - assigns a number according to the hour of timestamp of format HH:MM:SS (Note: this can be adapted in the code if date format changes)\n",
    "* Encrypt Freetext - uses Flair to identify Person, Organisation or Location entities in free text, replacing this data with the type of entity e.g. \"Clara goes on a walk\" -> \"<Person> goes on a walk\"\n",
    "* Encrypt Postcode - replaces postcode with LSOA data\n",
    "\n",
    "Actions if schema changes:\n",
    "* Replacing metadata.csv in bronze container of adlsinframricdev storage account with new schema, taken from catalogue.json in mc-data-catalogue repo https://github.com/M-RIC-TRE/mc-data-catalogue\n",
    "* Replacing PII_columns.csv with new PII columns in metadata.csv (replacing first column with \"other_identifiable_columns\" from metadata.csv and removing duplicates, then assigning each column manually to an encryption method in second column)\n",
    "* Adapting code in encrypt_postcode function to the relevant column(s), currently this is applied only to column \"ResidentPostcode\"\n",
    "\n",
    "Other risks:\n",
    "* Unresponsive kernel - from Flair library that loads a pre-trained Named Entity Recognition (NER) model, which could be computationally expensive and cause the kernel to become unresponsive if the text being analysed is too large or complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c71f974d-97cb-4356-9d51-895f0591e641",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e752b0d-b93e-4a31-85fe-09fd1d4e0fc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "## import hashlib for sha-256 encryption\n",
    "import hashlib\n",
    "\n",
    "# import packages for free text anonymisation\n",
    "import logging\n",
    "import presidio_analyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f886924a-7a24-4a86-9fcc-5d376667a2b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (1.21.5)\nCollecting numpy\n  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 56.0 MB/s eta 0:00:00\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-ef064bb0-a24c-4354-b576-a5c6bca7b893\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.11.1 requires pyspark<4,>=3.1.2, which is not installed.\nydata-profiling 4.1.0 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nnumba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.1.1 which is incompatible.\nSuccessfully installed numpy-1.24.3\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Needed to debug flair dependency\n",
    "\n",
    "%pip install numpy --upgrade\n",
    "\n",
    "# %pip install --force-reinstall numpy==1.22.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d6225d-91f2-4343-901e-11424f19eb7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b4900b-ae6e-4289-80fd-9598b008065c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import packages for free text anonymisation\n",
    "import flair\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951894e1-d08f-4c1b-b849-434bf4852f23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ingestion configuration\n",
    "NOTE: Any changes to the database schema and PII columns need to be adjusted in the csv files metadata.csv and PII_columns.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab907b87-cc61-42e7-9d6e-ed89aec22c10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#service_credential = dbutils.secrets.get(scope=\"flowehr-secrets\",key=\"<service-credential-key>\")\n",
    "\n",
    "client_id = dbutils.secrets.get(scope=\"flowehr-secrets\",key=\"flowehr-dbks-adls-app-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"flowehr-secrets\", key=\"flowehr-dbks-adls-app-secret\")\n",
    "\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adlsinframricdev.dfs.core.windows.net\", \"OAuth\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adlsinframricdev.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adlsinframricdev.dfs.core.windows.net\", client_id)\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adlsinframricdev.dfs.core.windows.net\", client_secret)\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adlsinframricdev.dfs.core.windows.net\", \"https://login.microsoftonline.com/71c35f80-28a0-4c98-8472-f29f22f6614d/oauth2/token\") # put your tenant id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be3e877-82af-49d5-a621-3b7b44162508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Set up the SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReadCSVFilesFromBlobStorage\").getOrCreate()\n",
    "\n",
    "# Set up the Azure Blob Storage account details\n",
    "account_name = \"adlsinframricdev\"\n",
    "account_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "661f4016-ad79-4277-9902-61394d292ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read metadata files\n",
    "\n",
    "# Each row corresponds to a table, with table name, column name and description, PII columns and free text columns, taken from catalogue repo\n",
    "df_metadata = spark.read.format(\"csv\").option(\"header\", True).load(\"abfs://bronze@adlsinframricdev.dfs.core.windows.net/metadata/metadata.csv\")\n",
    "# PII columns collated for all tables and assigned an encryption method, built manually\n",
    "df_PII_metadata = spark.read.format(\"csv\").option(\"header\", True).load(\"abfs://bronze@adlsinframricdev.dfs.core.windows.net/metadata/PII_columns.csv\")\n",
    "\n",
    "# Converting dataframe to dictionary to access columns by their encryption method\n",
    "rows = df_PII_metadata.collect()\n",
    "PII_META_DICT = {}\n",
    "for row in rows:\n",
    "    key = row[0]\n",
    "    value = row[1]\n",
    "    PII_META_DICT[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c96532d-b0d9-4d08-8771-3c689da3b605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[Column: string, Pseudo method: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PII_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6fb407-78ac-4e3f-8ee1-18cca9c7716c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Encrypt IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff06a4c-2689-4b92-a64a-14dd1b387ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encrypt_id(df_spark):\n",
    "    salt_key_name = 'saltkey-mric-pseudo-dev'\n",
    "    secretValue = 'v4a1REWsfIDCF6BwS3qzHiUXhUO1ahQ0ICl3mJ5QQyWVMUyS0mqm52Qffp3vDYPfVxGK1om09Jg1VOO5IzfTu9hFj2CUxl__ba-8UgPAnBuxKQI57O1FElkFFSPBLt6ARetQCEjjU0PpgIbwUb98BWzYg5bHuC4hY6eY3h14krcxSpsyzMHecne5bAwSmOrpEFLcoEJPNCujDd_fqMPfXkTq5J0TfWlGexR0TqMNpBKRw08py197AS8BhirXsmrrEfEIX9_hEOl3IB4FryRKFZOrsyvYcrtOnHhOFwk6tm0YK7Zhu_iBMgWC-vKYepmxF0VF556CzJ6lWm0i_qeJqQ'\n",
    "    # Creating a list of columns in PII metadata to be encrypted by this function\n",
    "    encrypt_ID_columns = []\n",
    "    # Loop over the items in the PII dictionary and check if the value is \"encrypt_ID\"\n",
    "    for key, value in PII_META_DICT.items():\n",
    "        if value == \"encrypt_ID\":\n",
    "            encrypt_ID_columns.append(key)\n",
    "    # Loop thorugh columns in the dataframe\n",
    "    for column in df_spark.columns:\n",
    "        # Finding columns in the dataframe to be encrypted by this function\n",
    "        if column in encrypt_ID_columns:\n",
    "            # Creating two new columns\n",
    "            salted_column_name = f'salted{column}'\n",
    "            encrypted_column_name = f'encrypted{column}'\n",
    "            # Concatinating the data in column with a salt key with secret value\n",
    "            df_spark = df_spark.withColumn(salted_column_name, concat(df_spark[column], lit(secretValue)))\n",
    "            # Encrypting the salted value with sha2 encryption\n",
    "            df_spark = df_spark.withColumn(encrypted_column_name, sha2(df_spark[salted_column_name].cast(\"Binary\"),256))\n",
    "            # Removing the column to leave the encrypted columns remaining\n",
    "            df_spark = df_spark.drop(column)\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "971fe143-c417-49fc-b8bc-7ec94b382e28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Encrypt Dates\n",
    "Removing the day from dates, NOTE: this relies on the data being of the format DD/MM/YYYY, but can be adapted in line 15 for other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321acf5a-eb4d-4a3c-8652-84d6badccab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7589cf3-012c-4372-b3eb-36535abd6322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encrypt_dates(df_spark):\n",
    "    # Creating a list of columns in PII metadata to be encrypted by this function\n",
    "    encrypt_date_columns = []\n",
    "    # Loop over the items in the dictionary and check if the value is \"encrypt_ID\"\n",
    "    for key, value in PII_META_DICT.items():\n",
    "        if value == \"encrypt_date\":\n",
    "            encrypt_date_columns.append(key)\n",
    "    # Loop thorugh columns in the dataframe\n",
    "    for column in df_spark.columns:\n",
    "        # Finding columns in the dataframe to be encrypted by this function\n",
    "        if column in encrypt_date_columns:\n",
    "            # Casting the data to string to be able to remove the \"day\" of the string\n",
    "            df_spark.withColumn(column,col(column).cast(StringType()))\n",
    "            # Removing day dates to leave month and year\n",
    "            df_spark = df_spark.withColumn(column, df_spark[column].substr(4,10))\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f86ac0-5d4b-4edf-ba15-a68c0fb4ee19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Encrypt Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05535abf-ae49-4b4a-94e1-bac5558c61a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encrypt_timestamps(df_spark):\n",
    "    \n",
    "    #Identify columns that need to be encrypted by this method\n",
    "    encrypt_timestamp_columns = []\n",
    "    # Loop over the items in the dictionary and check if the value is \"encrypt_ID\"\n",
    "    for key, value in PII_META_DICT.items():\n",
    "        if value == \"encrypt_timestamp\":\n",
    "            encrypt_timestamp_columns.append(key)\n",
    "    # # For dummy data\n",
    "    #encrypt_timestamp_columns.append(\"ActivityTimestamps\")\n",
    "\n",
    "    # create banded timestamp dataframe for banded timestamp calculation\n",
    "    data = [\n",
    "        (\"1\", \"00:00:00\"),\n",
    "        (\"2\", \"02:00:00\"),\n",
    "        (\"3\", \"04:00:00\"),\n",
    "        (\"4\", \"06:00:00\"),\n",
    "        (\"5\", \"08:00:00\"),\n",
    "        (\"6\", \"10:00:00\"),\n",
    "        (\"7\", \"12:00:00\"),\n",
    "        (\"8\", \"14:00:00\"),\n",
    "        (\"9\", \"16:00:00\"),\n",
    "        (\"10\", \"18:00,00\"),\n",
    "        (\"11\", \"20:00:00\"),\n",
    "        (\"12\", \"22:00:00\")\n",
    "    ]\n",
    "    schema = [\"id\",\"timestamp\"]\n",
    "    banded_df = spark.createDataFrame(\n",
    "        data,\n",
    "        schema\n",
    "    )\n",
    "    # prepare dataframe for timestamp\n",
    "    for column in df_spark.columns:\n",
    "        if column in encrypt_timestamp_columns:\n",
    "            encrypted_column = f'encrypted {column}'\n",
    "            df_spark = df_spark.withColumn(encrypted_column,\n",
    "                            when((df_spark[column] >  banded_df.collect()[0].timestamp) & (df_spark[column] <  banded_df.collect()[1].timestamp), \"1\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[1].timestamp) & (df_spark[column] <  banded_df.collect()[2].timestamp), \"2\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[2].timestamp) & (df_spark[column] <  banded_df.collect()[3].timestamp), \"3\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[3].timestamp) & (df_spark[column] <  banded_df.collect()[4].timestamp), \"4\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[4].timestamp) & (df_spark[column] <  banded_df.collect()[5].timestamp), \"5\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[5].timestamp) & (df_spark[column] <  banded_df.collect()[6].timestamp), \"6\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[6].timestamp) & (df_spark[column] <  banded_df.collect()[7].timestamp), \"7\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[7].timestamp) & (df_spark[column] <  banded_df.collect()[8].timestamp), \"8\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[8].timestamp) & (df_spark[column] <  banded_df.collect()[9].timestamp), \"9\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[9].timestamp) & (df_spark[column] <  banded_df.collect()[10].timestamp), \"10\")\n",
    "                            .when((df_spark[column] >  banded_df.collect()[10].timestamp) & (df_spark[column] <  banded_df.collect()[11].timestamp), \"11\")\n",
    "                            .otherwise(\"12\")\n",
    "            )\n",
    "            df_spark = df_spark.drop(column)\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf73640-98e6-4b98-a731-ecfbb148f5db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Encrypt free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286dbc40-c36f-4b20-b139-a9497b823ef0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb61369-669f-4581-93a7-85c23064828b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"presidio-analyzer\")\n",
    "\n",
    "ENTITIES = [\n",
    "    \"LOCATION\",\n",
    "    \"PERSON\",\n",
    "    \"ORGANIZATION\",\n",
    "]\n",
    "\n",
    "CHECK_LABEL_GROUPS = [\n",
    "        ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),\n",
    "        ({\"PERSON\"}, {\"PER\", \"PERSON\"}),\n",
    "        ({\"ORGANIZATION\"}, {\"ORG\"}),\n",
    "    ]\n",
    "\n",
    "MODEL_LANGUAGES = {\n",
    "        \"en\": \"flair/ner-english-large\",\n",
    "        \"es\": \"flair/ner-spanish-large\",\n",
    "        \"de\": \"flair/ner-german-large\",\n",
    "        \"nl\": \"flair/ner-dutch-large\",\n",
    "    }\n",
    "\n",
    "PRESIDIO_EQUIVALENCES = {\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"LOC\": \"LOCATION\",\n",
    "        \"ORG\": \"ORGANIZATION\",\n",
    "    }\n",
    "\n",
    "def check_label(entity,label,check_label_groups) -> bool:\n",
    "    return any(\n",
    "        [entity in egrp and label in lgrp for egrp, lgrp in check_label_groups]\n",
    "    )\n",
    "\n",
    "def convert_to_recognizer_result(entity):\n",
    "\n",
    "    entity_type = PRESIDIO_EQUIVALENCES.get(entity.tag, entity.tag)\n",
    "    start=entity.start_position\n",
    "    end=entity.end_position\n",
    "\n",
    "    return [entity_type,start,end]\n",
    "\n",
    "\n",
    "def encrypt(text,entity,start,end):\n",
    "    # Remove PII information based on start and end\n",
    "    # return f'{text[:start]}<{entity}>{text[start + len(entity) + (end-start):]}'\n",
    "    return f'{text[:start]}<{entity}>{text[end:]}'\n",
    "\n",
    "\n",
    "def run_encryption(text):\n",
    "    \n",
    "    # Clean text to remove html tags\n",
    "    text = re.sub('<.*?>','',text)\n",
    "\n",
    "    # Load model\n",
    "    model = SequenceTagger.load(MODEL_LANGUAGES.get(\"en\"))\n",
    "    sentences = Sentence(text)\n",
    "    model.predict(sentences)\n",
    "\n",
    "    # Iterative through text to determine PII entitites to be removed\n",
    "    results = []\n",
    "\n",
    "    for entity in ENTITIES:\n",
    "        if entity not in ENTITIES:\n",
    "            continue\n",
    "\n",
    "        for ent in sentences.get_spans(\"ner\"):\n",
    "            if not check_label(\n",
    "                entity, ent.labels[0].value, CHECK_LABEL_GROUPS\n",
    "            ):\n",
    "                continue\n",
    "            # Generate results of the PII entities identified, the start string position and end string position of entity\n",
    "            flair_result = convert_to_recognizer_result(ent)\n",
    "            results.append(flair_result)\n",
    "    \n",
    "    # Ensure the entities are ordered with start position descending\n",
    "    results = sorted(results, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    # Encrypt text to remove PII entities from text and replace with entity name\n",
    "    if len(results)!=0:\n",
    "        for PII_detection in results:\n",
    "            entity = PII_detection[0]\n",
    "            start = PII_detection[1]\n",
    "            end = PII_detection[2]\n",
    "            encrypted_text = encrypt(text,entity,start,end)\n",
    "            text = encrypted_text\n",
    "    else:\n",
    "        encrypted_text = \"None\"\n",
    "    return encrypted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40304f5-4560-4420-b2e3-3f7312227e13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encrypt_freetext(df_spark):\n",
    "    rows = df_metadata.collect()\n",
    "\n",
    "    encrypt_freetext_columns = []\n",
    "\n",
    "    # Loop over the rows and add the key-value pairs to the dictionary\n",
    "    for row in rows:\n",
    "        encrypt_freetext_columns.append(row['free_text_columns'])  \n",
    "        \n",
    "    encrypt_freetext_columns.append('FreeText')\n",
    "\n",
    "    for column in df_spark.columns:\n",
    "        if column in encrypt_freetext_columns:\n",
    "            encrypted_column = f'encrypted {column}'\n",
    "            free_text_anon_udf = udf(lambda x:run_encryption(str(x)),StringType())\n",
    "            df_spark = df_spark.withColumn(encrypted_column, free_text_anon_udf(col(column)))\n",
    "            df_spark = df_spark.drop(column)\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed955508-fa63-43db-9028-a2ea3e486adb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Encrypt postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6924820f-0107-40df-b358-44be67457deb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encrypt_postcode(df_spark):\n",
    "    #Identify columns that need to be encrypted by this method\n",
    "    encrypt_postcode_columns = []\n",
    "    # Loop over the items in the dictionary and check if the value is \"encrypt_ID\"\n",
    "    for key, value in PII_META_DICT.items():\n",
    "        if value == \"encrypt_address\":\n",
    "            encrypt_postcode_columns.append(key)\n",
    "\n",
    "    # Read files\n",
    "    df_gridlink = spark.read.format(\"csv\").option(\"header\", True).load(\"abfs://bronze@adlsinframricdev.dfs.core.windows.net/metadata/gridlink_header.csv\")\n",
    "    df_grid = spark.read.format(\"csv\").option(\"header\", True).load(\"abfs://bronze@adlsinframricdev.dfs.core.windows.net/metadata/gridall.csv\")\n",
    "    columns = df_gridlink.columns\n",
    "    columns = ','.join(columns)\n",
    "    # Merging both CSVs\n",
    "    df_grid_all = df_gridlink.union(df_grid)\n",
    "    reduced_df_grid_all = df_grid_all[['PCDS','LSOA01']]\n",
    "    # There is only one column in the catalogue to be encrypted by this method, join LSOA with this postcode to encrypt\n",
    "    for column in df_spark.columns:\n",
    "        if column in encrypt_postcode_columns:\n",
    "            df_final = df_spark.join(reduced_df_grid_all, df_spark[\"ResidentPostcode\"]==reduced_df_grid_all[\"PCDS\"])\n",
    "            df_final = df_final.drop(\"ResidentPostcode\")\n",
    "        else:\n",
    "            df_final = df_spark\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60fea4b-3f2c-465a-ba93-d4bba8b34ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pseudo function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0e709d-59d2-401b-85f5-87e4331fb72a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline of pseudo functions that encrypts the table according to the table columns and PII_metadata\n",
    "def pseudo(df_spark):        \n",
    "    df_spark = encrypt_id(df_spark)\n",
    "    df_spark = encrypt_dates(df_spark)\n",
    "    df_spark = encrypt_timestamps(df_spark)\n",
    "    df_spark = encrypt_freetext(df_spark)\n",
    "    df_spark = encrypt_postcode(df_spark)\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cede591-a3b6-49d5-9ac5-d5c222931777",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run pipeline for dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49167e12-4bfa-41e1-b216-080cd2432188",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# dummy_df = spark.read.format(\"csv\").option(\"header\", True).load(f\"abfs://bronze@adlsinframricdev.dfs.core.windows.net/metadata/dummyPIDData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580aa719-8b21-4dae-a593-fe12bffeaed4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dummy_df = pseudo(dummy_df)\n",
    "\n",
    "# dummy_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bb5cb9-44cc-49a4-98f5-28d7e80110cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (dummy_df\n",
    "# .write.format(\"com.databricks.spark.csv\")\n",
    "# .format(\"overwrite\")\n",
    "# .option(\"header\", \"true\")\n",
    "# .save(f\"abfs://silver@adlsinframricdev.dfs.core.windows.net/dummy_outputs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd0e5a2-d719-4795-b49c-e65c6ef35cd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dummy_df = spark.read.format(\"csv\").option(\"header\", True).load(f\"abfs://silver@adlsinframricdev.dfs.core.windows.net/dummy_outputs.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411858a1-f0fa-4e52-8584-8525a88ea981",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run pipeline for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1d188a-b6fb-46ab-b9cd-21a70eaa31e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_metadata = spark.read.format(\"csv\").option(\"header\", True).load(\"abfs://bronze@adlsinframricdev.dfs.core.windows.net/metadata/metadata.csv\")\n",
    "\n",
    "# Creating the list of tables from metadata dataframe\n",
    "df_metadata_rows = df_metadata.collect()\n",
    "\n",
    "table_list = []\n",
    "for row in df_metadata_rows:\n",
    "    table_list.append(row[0])\n",
    "\n",
    "table_list = [t.replace(\"dbo.\",\"\") for t in table_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7853f138-afd3-44cd-99ab-8ed2461fb2e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Iterating through each file in bronze, running the function and writing to silver\n",
    "for table in table_list:\n",
    "    df = spark.read.format(\"parquet\").option(\"header\", True).load(f\"abfs://bronze@adlsinframricdev.dfs.core.windows.net/raw/{table}.parquet\")\n",
    "    df = pseudo(df)\n",
    "    output_filename = f'{table}.encrypted'\n",
    "    (df.coalesce(1)\n",
    "    .write.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(f\"abfs://silver@adlsinframricdev.dfs.core.windows.net/transformed/{output_filename}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6400ebcf-a72e-4770-8477-8d9e70398bae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'UBRN': 'encrypt_ID',\n",
       " 'USRN': 'encrypt_ID',\n",
       " 'ClientID': 'encrypt_ID',\n",
       " 'ContactTransportID': 'encrypt_ID',\n",
       " 'ContactInterpreterID': 'encrypt_ID',\n",
       " 'StartUserID': 'encrypt_ID',\n",
       " 'EndReason': 'encrypt_ID',\n",
       " 'UserID': 'encrypt_ID',\n",
       " 'EnteredBy': 'encrypt_timestamp',\n",
       " 'RemovalUserID': 'encrypt_ID',\n",
       " 'Path': 'encrypt_ID',\n",
       " 'Author': 'encrypt_ID',\n",
       " 'NNN': 'encrypt_ID',\n",
       " 'NNNStatus': 'encrypt_ID',\n",
       " 'AlternativeID': 'encrypt_ID',\n",
       " 'Surname': 'encrypt_ID',\n",
       " 'SurnameSoundex': 'encrypt_ID',\n",
       " 'Firstname': 'encrypt_ID',\n",
       " 'FirstnameSoundex': 'encrypt_ID',\n",
       " 'Title': 'encrypt_ID',\n",
       " 'DateOfBirth': 'encrypt_date',\n",
       " 'EstimatedDOB': 'encrypt_date',\n",
       " 'DaytimePhone': 'encrypt_ID',\n",
       " 'EveningPhone': 'encrypt_ID',\n",
       " 'MotherLink': 'encrypt_ID',\n",
       " 'FatherLink': 'encrypt_ID',\n",
       " 'EMailAddress': 'encrypt_ID',\n",
       " 'MobilePhone': 'encrypt_ID',\n",
       " 'MainCarer': 'encrypt_ID',\n",
       " 'NINumber': 'encrypt_ID',\n",
       " 'NNNLastTape': 'encrypt_ID',\n",
       " 'OtherCarer': 'encrypt_ID',\n",
       " 'SpineID': 'encrypt_ID',\n",
       " 'SupersedingNNN': 'encrypt_ID',\n",
       " 'ResidentPostcode': 'encrypt_address',\n",
       " 'PresBy': 'encrypt_date',\n",
       " 'ClientNameID': 'encrypt_ID',\n",
       " 'Prefix': 'encrypt_ID',\n",
       " 'Suffix': 'encrypt_ID',\n",
       " 'GivenName1': 'encrypt_ID',\n",
       " 'GivenName2': 'encrypt_ID',\n",
       " 'GivenName3': 'encrypt_ID',\n",
       " 'GivenName4': 'encrypt_ID',\n",
       " 'GivenName5': 'encrypt_ID',\n",
       " 'GivenName1Soundex': 'encrypt_ID',\n",
       " 'GivenName2Soundex': 'encrypt_ID',\n",
       " 'GivenName3Soundex': 'encrypt_ID',\n",
       " 'GivenName4Soundex': 'encrypt_ID',\n",
       " 'GivenName5Soundex': 'encrypt_ID',\n",
       " 'EntryBy': 'encrypt_date',\n",
       " 'RemovalBy': 'encrypt_date',\n",
       " 'Owner': 'encrypt_ID',\n",
       " 'UpdatedBy': 'encrypt_date',\n",
       " 'ReportedBy': 'encrypt_date',\n",
       " 'Name': 'encrypt_ID',\n",
       " 'WitnessingHCP': 'encrypt_ID',\n",
       " 'DeletedBy': 'encrypt_date',\n",
       " 'ImagePath': 'encrypt_ID',\n",
       " 'TelephoneNumber': 'encrypt_ID',\n",
       " 'Forename': 'encrypt_ID',\n",
       " 'PhoneNumber': 'encrypt_ID',\n",
       " 'FaxNumber': 'encrypt_ID',\n",
       " 'Fax': 'encrypt_ID',\n",
       " 'Email': 'encrypt_ID',\n",
       " 'ContactPhone': 'encrypt_ID',\n",
       " 'ContactName': 'encrypt_ID',\n",
       " 'FirstName': 'encrypt_ID',\n",
       " 'AddressLine1': 'encrypt_ID',\n",
       " 'AddressLine2': 'encrypt_ID',\n",
       " 'AddressLine3': 'encrypt_ID',\n",
       " 'AddressLine4': 'encrypt_ID',\n",
       " 'AddressLine5': 'encrypt_ID',\n",
       " 'Phone': 'encrypt_ID',\n",
       " 'DeviceID': 'encrypt_ID',\n",
       " 'Initials': 'encrypt_ID',\n",
       " 'UserNumber': 'encrypt_ID',\n",
       " 'GenUserID': 'encrypt_ID',\n",
       " 'Password': 'encrypt_ID',\n",
       " 'UserTypeID': 'encrypt_ID',\n",
       " 'PasswordExpDate': 'encrypt_date',\n",
       " 'Disabled': 'encrypt_ID',\n",
       " 'OrganisationType': 'encrypt_ID',\n",
       " 'LoginID': 'encrypt_ID',\n",
       " 'UserPassword': 'encrypt_ID',\n",
       " 'OwnNotes': 'encrypt_ID',\n",
       " 'StudentNotes': 'encrypt_ID',\n",
       " 'EditableLetterAccess': 'encrypt_ID',\n",
       " 'DocListAccess': 'encrypt_ID',\n",
       " 'ChildRecordAccess': 'encrypt_ID',\n",
       " 'WhiteListActive': 'encrypt_ID',\n",
       " 'LastSessionID': 'encrypt_ID',\n",
       " 'NoteType': 'encrypt_ID',\n",
       " 'SubNoteType': 'encrypt_ID',\n",
       " 'StartUpPage': 'encrypt_ID',\n",
       " 'UserIdIsSpineId': 'encrypt_ID',\n",
       " 'SystemAccount': 'encrypt_ID',\n",
       " 'LRExemptionEnabled': 'encrypt_ID',\n",
       " 'WardManager': 'encrypt_ID',\n",
       " 'OCRequestAuthorisationLevel': 'encrypt_ID',\n",
       " 'SAFLoginID': 'encrypt_ID',\n",
       " 'MainRole': 'encrypt_ID',\n",
       " 'AdditionalRole1': 'encrypt_ID',\n",
       " 'AdditionalRole2': 'encrypt_ID',\n",
       " 'PayrollNumber': 'encrypt_ID',\n",
       " 'OtherTrustEmployee': 'encrypt_ID',\n",
       " 'TrustUse1': 'encrypt_ID',\n",
       " 'TrustUse2': 'encrypt_ID',\n",
       " 'TrustUse3': 'encrypt_ID',\n",
       " 'TrustUse4': 'encrypt_ID',\n",
       " 'AdditionalDate1': 'encrypt_date',\n",
       " 'LastViewedWard': 'encrypt_date',\n",
       " 'Salt': 'encrypt_ID',\n",
       " 'PasswordType': 'encrypt_ID',\n",
       " 'PasswordAttempts': 'encrypt_ID',\n",
       " 'MaintenanceMode': 'encrypt_ID',\n",
       " 'DischargeAddressLine1': 'encrypt_ID',\n",
       " 'DischargeAddressLine2': 'encrypt_ID',\n",
       " 'DischargeAddressLine3': 'encrypt_ID',\n",
       " 'DischargeAddressLine4': 'encrypt_ID',\n",
       " 'DischargeAddressLine5': 'encrypt_ID',\n",
       " 'Referrer': 'encrypt_ID',\n",
       " 'Filename': 'encrypt_ID'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PII_META_DICT"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "shir_bronze_silver_anonymise",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
